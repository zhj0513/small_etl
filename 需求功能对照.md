# 需求功能对照文档

本文档对照培训项目需求与代码实现，验证各项需求的满足情况。

---

## 需求 1：SQLModel 定义表结构 + Alembic 管理表结构

### 状态：✅ 已满足

### 代码位置

| 功能 | 文件路径 | 行号 | 说明 |
|------|----------|------|------|
| Asset 表定义 | `src/small_etl/domain/models.py` | 10-31 | 使用 `SQLModel, table=True` 定义资产表 |
| Trade 表定义 | `src/small_etl/domain/models.py` | 34-69 | 使用 `SQLModel, table=True` 定义交易表，包含外键约束 |
| Alembic 环境配置 | `alembic/env.py` | 1-72 | Alembic 迁移环境，集成 Hydra 配置读取数据库连接 |
| 初始迁移脚本 | `alembic/versions/001_initial.py` | - | 创建 asset 和 trade 表 |
| 约束迁移脚本 | `alembic/versions/002_add_unique_constraints.py` | - | 添加唯一约束 |
| 外键迁移脚本 | `alembic/versions/96f12c403d72_add_foreign_key_constraint_for_trade_.py` | - | 添加 Trade.account_id → Asset.account_id 外键 |

### 关键代码示例

```python
# src/small_etl/domain/models.py:10-31
class Asset(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    account_id: str = Field(unique=True, index=True, max_length=20)
    account_type: int = Field(index=True)
    cash: Decimal = Field(max_digits=20, decimal_places=2)
    frozen_cash: Decimal = Field(max_digits=20, decimal_places=2)
    market_value: Decimal = Field(max_digits=20, decimal_places=2)
    total_asset: Decimal = Field(max_digits=20, decimal_places=2)
    updated_at: datetime
```

---

## 需求 2：DuckDB 负责 CSV 的提取和校验 + postgres 插件写入数据库

### 状态：✅ 已满足

### 代码位置

| 功能 | 文件路径 | 行号 | 说明 |
|------|----------|------|------|
| DuckDB 客户端 | `src/small_etl/data_access/duckdb_client.py` | 14-361 | DuckDB 内存数据库操作封装 |
| CSV 加载 | `src/small_etl/data_access/duckdb_client.py` | 103-119 | `load_csv_bytes()` 方法加载 CSV 到 DuckDB 表 |
| PostgreSQL 插件挂载 | `src/small_etl/data_access/duckdb_client.py` | 26-42 | `attach_postgres()` 方法安装并挂载 postgres 插件 |
| Upsert 写入数据库 | `src/small_etl/data_access/duckdb_client.py` | 44-101 | `upsert_to_postgres()` 使用 UPDATE + INSERT 策略 |
| Extractor DuckDB 模式 | `src/small_etl/services/extractor.py` | 119-133 | DuckDB 自动提取模式 |

### 关键代码示例

```python
# src/small_etl/data_access/duckdb_client.py:26-42
def attach_postgres(self, database_url: str) -> None:
    if self._pg_attached:
        return
    self._conn.execute("INSTALL postgres")
    self._conn.execute("LOAD postgres")
    self._conn.execute(f"ATTACH '{database_url}' AS pg (TYPE POSTGRES)")
    self._pg_attached = True

# src/small_etl/data_access/duckdb_client.py:75-94 (UPDATE + INSERT 策略)
update_sql = f"""
    UPDATE pg.{table_name} AS p
    SET {set_clause}
    FROM {temp_table} AS t
    WHERE p.{conflict_column} = t."{conflict_column}"
"""
insert_sql = f"""
    INSERT INTO pg.{table_name} ({columns_str})
    SELECT {values_str} FROM {temp_table} t
    WHERE NOT EXISTS (
        SELECT 1 FROM pg.{table_name} p
        WHERE p.{conflict_column} = t."{conflict_column}"
    )
"""
```

---

## 需求 3：使用 AI 工具辅助完成，附带完整的 Agent 对话记录和需求文档

### 状态：✅ 已满足

### 代码位置

| 功能 | 文件路径 | 说明 |
|------|----------|------|
| 产品需求文档 | `doc/product.md` | 产品引导文档，包含目标用户、功能、使用场景 |
| 技术规范文档 | `doc/tech.md` | 技术规范文档，包含技术栈、约束、规范 |
| 架构设计文档 | `doc/architecture.md` | 详细架构设计，包含分层、组件、数据流 |
| Claude 指导文件 | `CLAUDE.md` | AI 工具协作指导文件 |
| 本对照文档 | `需求功能对照.md` | 需求与代码对照 |

---

## 需求 4：通过 pyright、pyrefly、ruff 静态检查

### 状态：✅ 已满足

### 代码位置

| 功能 | 文件路径 | 行号 | 说明 |
|------|----------|------|------|
| Ruff 配置 | `pyproject.toml` | 46-58 | 配置 line-length=180, target-version=py312 |
| Pyright 配置 | `pyproject.toml` | 60-65 | 配置 typeCheckingMode=standard |
| Pyrefly 配置 | `pyproject.toml` | 67-77 | 配置项目 includes 和错误级别 |
| 工具依赖 | `pyproject.toml` | 34-36 | ruff>=0.14.8, pyright>=1.1.407, pyrefly>=0.45.2 |

### 运行命令

```bash
pixi run ruff check src/      # Lint 检查
pixi run pyright src/         # 类型检查
pixi run pyrefly check src/   # 类型检查（备选）
```

### 关键配置

```toml
# pyproject.toml:46-58
[tool.ruff]
line-length = 180
target-version = "py312"
src = ["src"]

[tool.ruff.lint]
select = ["E", "F", "I"]

# pyproject.toml:60-65
[tool.pyright]
include = ["src"]
typeCheckingMode = "standard"
pythonVersion = "3.12"
```

---

## 需求 5：Hydra 定义 CSV 格式和字段转换规则

### 状态：✅ 已满足

### 代码位置

| 功能 | 文件路径 | 行号 | 说明 |
|------|----------|------|------|
| Extractor 配置文件 | `configs/extractor/default.yaml` | 1-110 | 定义 assets/trades 的列映射和类型转换规则 |
| 主配置入口 | `configs/config.yaml` | 1-12 | Hydra 主配置，引用 extractor 默认配置 |
| 配置驱动提取 | `src/small_etl/services/extractor.py` | 111-117 | 检测配置并使用 Polars 进行类型转换 |
| DataFrame 转换 | `src/small_etl/services/extractor.py` | 57-96 | `_transform_dataframe()` 实现列映射 |
| Polars 类型映射 | `src/small_etl/services/extractor.py` | 39-55 | `_get_polars_dtype()` 配置类型到 Polars 类型映射 |

### 关键配置

```yaml
# configs/extractor/default.yaml:4-38
assets:
  columns:
    - name: account_id
      csv_name: account_id
      dtype: Utf8
      nullable: false
    - name: cash
      csv_name: cash
      dtype: Float64
      nullable: false
    - name: updated_at
      csv_name: updated_at
      dtype: Datetime
      format: "%Y-%m-%dT%H:%M:%S%.f"
      nullable: false
```

### 关键代码

```python
# src/small_etl/services/extractor.py:111-117
if self._config is not None and hasattr(self._config, "assets"):
    df = pl.read_csv(io.BytesIO(csv_bytes), has_header=True, infer_schema_length=10000)
    df = self._transform_dataframe(df, self._config.assets.columns)
```

---

## 需求 6：DuckDB 负责统计分析

### 状态：✅ 已满足

### 代码位置

| 功能 | 文件路径 | 行号 | 说明 |
|------|----------|------|------|
| Asset 统计查询 | `src/small_etl/data_access/duckdb_client.py` | 168-239 | `query_asset_statistics()` 从 PostgreSQL 查询资产统计 |
| Trade 统计查询 | `src/small_etl/data_access/duckdb_client.py` | 241-350 | `query_trade_statistics()` 从 PostgreSQL 查询交易统计 |
| Analytics 服务 | `src/small_etl/services/analytics.py` | - | 封装统计服务，调用 DuckDBClient |
| Pipeline 集成 | `src/small_etl/application/pipeline.py` | 154-156 | Pipeline 中调用统计分析 |

### 关键代码

```python
# src/small_etl/data_access/duckdb_client.py:181-191
overall_sql = """
    SELECT
        COUNT(*) as total_records,
        COALESCE(SUM(cash), 0) as total_cash,
        COALESCE(SUM(total_asset), 0) as total_assets,
        COALESCE(AVG(total_asset), 0) as avg_total_asset
    FROM pg.asset
"""

# src/small_etl/data_access/duckdb_client.py:206-216 (按 account_type 分组)
by_type_sql = """
    SELECT
        account_type,
        COUNT(*) as count,
        COALESCE(SUM(total_asset), 0) as sum_total,
        COALESCE(AVG(total_asset), 0) as avg_total
    FROM pg.asset
    GROUP BY account_type
"""
```

---

## 需求 7：合理的类设计 + 单元测试

### 状态：✅ 已满足

### 类设计 - 分层架构

| 层 | 目录 | 主要类 |
|----|------|--------|
| Domain | `src/small_etl/domain/` | Asset, Trade, AssetSchema, TradeSchema |
| Data Access | `src/small_etl/data_access/` | DuckDBClient, PostgresRepository, S3Connector |
| Services | `src/small_etl/services/` | ExtractorService, ValidatorService, LoaderService, AnalyticsService |
| Application | `src/small_etl/application/` | ETLPipeline |

### 单元测试位置

| 测试文件 | 路径 | 覆盖内容 |
|----------|------|----------|
| 模型测试 | `tests/unit/test_models.py` | Asset, Trade 模型 |
| 验证器测试 | `tests/unit/test_validator.py` | ValidatorService, Pandera Schema |
| DuckDB 测试 | `tests/unit/test_duckdb.py` | DuckDBClient |
| 分析服务测试 | `tests/unit/test_analytics.py` | AnalyticsService |
| Pipeline 测试 | `tests/unit/test_pipeline.py` | ETLPipeline |
| 加载器测试 | `tests/unit/test_loader.py` | LoaderService |
| 提取器测试 | `tests/unit/test_extractor.py` | ExtractorService |
| 仓库测试 | `tests/unit/test_postgres_repository.py` | PostgresRepository |
| CLI 测试 | `tests/unit/test_cli.py` | 命令行接口 |
| 调度器测试 | `tests/unit/test_scheduler.py` | APScheduler 集成 |
| 集成测试 | `tests/integration/test_full_pipeline.py` | 端到端流程 |

### 运行命令

```bash
pixi run pytest tests/unit/ -v --no-cov     # 运行单元测试
pixi run pytest tests/ -v --cov=src/small_etl  # 运行全部测试+覆盖率
```

---

## 需求 8：pyproject.toml + pixi 管理配置

### 状态：✅ 已满足

### 代码位置

| 功能 | 文件路径 | 行号 | 说明 |
|------|----------|------|------|
| 项目元数据 | `pyproject.toml` | 1-6 | 项目名、版本、作者 |
| Pixi 工作空间 | `pyproject.toml` | 8-11 | channels, platforms 配置 |
| PyPI 依赖 | `pyproject.toml` | 13-26 | minio, sqlmodel, duckdb, polars, pandera, hydra 等 |
| Conda 依赖 | `pyproject.toml` | 29-36 | python, pytest, ruff, pyright, pyrefly |
| Pytest 配置 | `pyproject.toml` | 38-44 | 测试路径和选项 |
| Ruff/Pyright/Pyrefly | `pyproject.toml` | 46-77 | 静态检查工具配置 |

### 关键配置

```toml
# pyproject.toml:13-25
[tool.pixi.pypi-dependencies]
minio = ">=7.2.9"
sqlmodel = ">=0.0.22"
alembic = ">=1.13.0"
duckdb = ">=1.1.0"
polars = ">=1.0.0"
pyarrow = ">=17.0.0"
pandera = { version = ">=0.20.0", extras = ["polars"] }
hydra-core = ">=1.3.0"
psycopg2-binary = ">=2.9.0"
apscheduler = ">=3.10.0"

# pyproject.toml:29-36
[tool.pixi.dependencies]
python = "3.12.*"
pytest = ">=9.0.2,<10"
pytest-cov = ">=7.0.0,<8"
ruff = ">=0.14.8,<0.15"
pyright = ">=1.1.407,<2"
pyrefly = ">=0.45.2,<0.46"
```

---

## 需求 9：Podman 管理 PostgreSQL 和 S3 镜像 + pytest 管理测试

### 状态：✅ 已满足

### 代码位置

| 功能 | 文件路径 | 行号 | 说明 |
|------|----------|------|------|
| 容器管理模块 | `tests/container_manager.py` | 1-306 | Podman 容器生命周期管理 |
| PostgreSQL 容器配置 | `tests/container_manager.py` | 15-20 | 镜像、端口、用户配置 |
| MinIO 容器配置 | `tests/container_manager.py` | 22-28 | 镜像、端口、访问密钥配置 |
| 启动 PostgreSQL | `tests/container_manager.py` | 85-108 | `start_postgres_container()` |
| 启动 MinIO | `tests/container_manager.py` | 111-135 | `start_minio_container()` |
| pytest fixtures | `tests/conftest.py` | 255-292 | `postgres_container`, `minio_container` fixtures |
| 测试数据库 engine | `tests/conftest.py` | 45-77 | `test_db_engine` fixture |

### 关键配置

```python
# tests/container_manager.py:15-28
POSTGRES_CONTAINER = "test-postgres"
POSTGRES_IMAGE = "docker.io/library/postgres:15"
POSTGRES_PORT = 15433
POSTGRES_USER = "etl"
POSTGRES_PASSWORD = "etlpass"
POSTGRES_DB = "etl_test_db"

MINIO_CONTAINER = "test-minio"
MINIO_IMAGE = "docker.io/minio/minio:latest"
MINIO_PORT = 19010
MINIO_ACCESS_KEY = "minioadmin"
MINIO_SECRET_KEY = "minioadmin123"
```

### pytest fixtures

```python
# tests/conftest.py:255-266
@pytest.fixture(scope="session")
def postgres_container():
    """Start PostgreSQL container and stop it after tests."""
    start_postgres_container()
    if not wait_for_postgres():
        pytest.fail("PostgreSQL container failed to start")
    yield
    stop_container(POSTGRES_CONTAINER)

# tests/conftest.py:269-283
@pytest.fixture(scope="session")
def minio_container():
    """Start MinIO container and stop it after tests."""
    start_minio_container()
    if not wait_for_minio():
        pytest.fail("MinIO container failed to start")
    setup_minio_bucket("fake-data-for-training")
    copy_test_data_from_dev("fake-data-for-training")
    yield
    stop_container(MINIO_CONTAINER)
```

---

## 总结

| 需求编号 | 需求描述 | 状态 |
|----------|----------|------|
| 1 | SQLModel + Alembic 表结构管理 | ✅ 已满足 |
| 2 | DuckDB CSV 提取校验 + postgres 插件写入 | ✅ 已满足 |
| 3 | AI 工具 + Agent 记录 + 需求文档 | ✅ 已满足 |
| 4 | pyright/pyrefly/ruff 静态检查 | ✅ 已满足 |
| 5 | Hydra CSV 格式定义和字段转换 | ✅ 已满足 |
| 6 | DuckDB 统计分析 | ✅ 已满足 |
| 7 | 合理类设计 + 单元测试 | ✅ 已满足 |
| 8 | pyproject.toml + pixi 配置管理 | ✅ 已满足 |
| 9 | Podman PostgreSQL/S3 + pytest 测试 | ✅ 已满足 |

**所有 9 项需求均已满足。**

---

在src/small_etl/domain/schemas.py中，运用pandera校验dataframe schema

---

## 可扩展性：添加新类型 CSV 文件指南

本项目采用**数据类型注册表模式**，添加新的 CSV 数据类型（如 Order 订单）只需修改以下几个位置：

### 必须修改的文件

| 序号 | 文件路径 | 修改内容 | 说明 |
|------|----------|----------|------|
| 1 | `src/small_etl/domain/models.py` | 添加 SQLModel 类 | 定义数据库表结构 |
| 2 | `src/small_etl/domain/schemas.py` | 添加 Pandera Schema | 定义数据验证规则 |
| 3 | `src/small_etl/domain/registry.py` | 注册数据类型配置 | 在 `_register_default_types()` 中添加配置 |
| 4 | `alembic/versions/` | 添加迁移脚本 | 创建数据库表 |

### 可选修改的文件

| 序号 | 文件路径 | 修改内容 | 说明 |
|------|----------|----------|------|
| 5 | `configs/extractor/default.yaml` | 添加列映射配置 | 可选：自定义 CSV 列名到字段名映射 |
| 6 | `configs/s3/dev.yaml` | 添加 S3 文件路径配置 | 配置新 CSV 文件路径 |
| 7 | `src/small_etl/services/analytics.py` | 添加统计方法 | 可选：自定义统计分析逻辑 |
| 8 | `src/small_etl/data_access/duckdb_client.py` | 添加统计查询 | 可选：自定义 DuckDB 统计 SQL |

---

### 步骤 1：定义 SQLModel 模型

```python
# src/small_etl/domain/models.py

class Order(SQLModel, table=True):
    """订单表模型."""
    id: Optional[int] = Field(default=None, primary_key=True)
    order_id: str = Field(unique=True, index=True, max_length=50)
    account_id: str = Field(foreign_key="asset.account_id", max_length=20)
    stock_code: str = Field(max_length=10)
    order_price: Decimal = Field(max_digits=20, decimal_places=2)
    order_volume: int
    order_status: int  # 订单状态枚举
    created_at: datetime
    updated_at: datetime
```

---

### 步骤 2：定义 Pandera 验证 Schema

```python
# src/small_etl/domain/schemas.py

class OrderSchema(pa.DataFrameModel):
    """订单数据验证 Schema."""
    order_id: str = pa.Field(nullable=False)
    account_id: str = pa.Field(nullable=False)
    stock_code: str = pa.Field(nullable=False)
    order_price: float = pa.Field(ge=0)
    order_volume: int = pa.Field(gt=0)
    order_status: int = pa.Field(isin=[0, 1, 2, 3])  # 待处理/已成交/已撤销/部分成交
    created_at: datetime = pa.Field(nullable=False)
    updated_at: datetime = pa.Field(nullable=False)

    class Config:
        strict = True
        coerce = True
```

---

### 步骤 3：在注册表中注册数据类型配置

```python
# src/small_etl/domain/registry.py - 在 _register_default_types() 函数中添加

def _register_default_types() -> None:
    from small_etl.domain.models import Asset, Trade, Order  # 添加 Order
    from small_etl.domain.schemas import AssetSchema, TradeSchema, OrderSchema  # 添加 OrderSchema

    # ... 已有的 asset 和 trade 配置 ...

    # Order 配置
    DataTypeRegistry.register(
        DataTypeConfig(
            name="order",                    # 数据类型名称
            table_name="order",              # PostgreSQL 表名
            unique_key="order_id",           # 唯一键（用于 upsert）
            db_columns=[                     # 数据库列列表
                "order_id",
                "account_id",
                "stock_code",
                "order_price",
                "order_volume",
                "order_status",
                "created_at",
                "updated_at",
            ],
            s3_file_key="orders_file",       # S3 配置中的文件路径键名
            raw_table_name="raw_orders",     # DuckDB 临时表名
            extract_sql="""                  # DuckDB 提取 SQL
                SELECT
                    id,
                    order_id,
                    CAST(account_id AS VARCHAR) as account_id,
                    stock_code,
                    CAST(order_price AS DECIMAL(20, 2)) as order_price,
                    order_volume,
                    order_status,
                    created_at,
                    updated_at
                FROM {table}
            """,
            model_class=Order,
            schema_class=OrderSchema,
            foreign_key_column="account_id",      # 外键列（可选）
            foreign_key_reference="asset",        # 外键引用的数据类型（可选）
            statistics_config={                   # 统计配置（可选）
                "aggregates": {
                    "order_volume": ["sum", "avg"],
                    "order_price": ["avg"],
                },
                "group_by": ["order_status", "stock_code"],
            },
        )
    )
```

---

### 步骤 4：创建数据库迁移脚本

```bash
# 自动生成迁移脚本
pixi run alembic revision --autogenerate -m "add order table"

# 应用迁移
pixi run alembic upgrade head
```

---

### 步骤 5（可选）：添加 Hydra 配置

```yaml
# configs/extractor/default.yaml - 添加 orders 配置

orders:
  columns:
    - name: order_id
      csv_name: order_id
      dtype: Utf8
      nullable: false
    - name: account_id
      csv_name: account_id
      dtype: Utf8
      nullable: false
    - name: order_price
      csv_name: order_price
      dtype: Float64
      nullable: false
    - name: order_volume
      csv_name: order_volume
      dtype: Int64
      nullable: false
    - name: order_status
      csv_name: order_status
      dtype: Int32
      nullable: false
    - name: created_at
      csv_name: created_at
      dtype: Datetime
      format: "%Y-%m-%dT%H:%M:%S%.f"
      nullable: false
    - name: updated_at
      csv_name: updated_at
      dtype: Datetime
      format: "%Y-%m-%dT%H:%M:%S%.f"
      nullable: false
```

```yaml
# configs/s3/dev.yaml - 添加 orders_file

orders_file: "orders.csv"
```

---

### 使用新数据类型

注册完成后，可直接使用通用方法处理新数据类型：

```python
# 提取
orders_df = extractor.extract(bucket, "orders.csv", "order")

# 验证
validation_result = validator.validate(orders_df, "order", valid_account_ids)

# 加载
load_result = loader.load(validation_result.valid_rows, "order")
```

---

### 架构设计说明

本项目采用**数据类型注册表模式**（`DataTypeRegistry`）实现可扩展性：

```
┌─────────────────────────────────────────────────────────────┐
│                    DataTypeRegistry                          │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐          │
│  │   asset     │  │   trade     │  │   order     │  ...     │
│  │  config     │  │  config     │  │  config     │          │
│  └─────────────┘  └─────────────┘  └─────────────┘          │
└─────────────────────────────────────────────────────────────┘
         │                 │                 │
         ▼                 ▼                 ▼
┌─────────────────────────────────────────────────────────────┐
│              通用服务方法 (Generic Methods)                  │
│  ExtractorService.extract(df, data_type)                    │
│  ValidatorService.validate(df, data_type)                   │
│  LoaderService.load(df, data_type)                          │
└─────────────────────────────────────────────────────────────┘
```

**核心配置类 `DataTypeConfig` 包含：**

| 属性 | 类型 | 说明 |
|------|------|------|
| `name` | str | 数据类型唯一标识 |
| `table_name` | str | PostgreSQL 表名 |
| `unique_key` | str | Upsert 冲突检测列 |
| `db_columns` | list[str] | 数据库列列表 |
| `s3_file_key` | str | S3 配置中的文件路径键 |
| `raw_table_name` | str | DuckDB 临时表名 |
| `extract_sql` | str | DuckDB 提取 SQL 模板 |
| `model_class` | type | SQLModel 模型类 |
| `schema_class` | type | Pandera Schema 类 |
| `foreign_key_column` | str | 外键列名（可选） |
| `foreign_key_reference` | str | 外键引用数据类型（可选） |
| `statistics_config` | dict | 统计配置（可选） |

---
